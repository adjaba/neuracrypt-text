# -*- coding: utf-8 -*-
"""master distribution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UAUh6jr2f6myhwJFCAtmgmrtrvOCHBZR

# Some Parameters
"""

vocab_size = 10000
train_split = 0.45
test_split = 0.45
validation_split = 0.1
eat_ram = False
epochs = 100
minlen = 5

"""# Importing Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import string
from collections import Counter
import os
import time
import tqdm
# %matplotlib inline

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# from nltk.stem import SnowballStemmer
# from nltk.corpus import stopwords

import torch
from torch import nn
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
import torch.optim as optim

import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Activation 
from tensorflow.keras.metrics import categorical_accuracy
from tensorflow.keras.layers import GlobalMaxPool1D, BatchNormalization, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, BatchNormalization

"""# Loading dataset"""

from os import path
if not path.exists('./spam_clean.csv'):
    import gdown
    drive_url = "https://drive.google.com/u/0/uc?id=1wD2jVTiCV7hmH05KxejsiYZHb3wBh1rM&export=download"
    output = "./spam_clean.csv"
    gdown.download(drive_url, output)

sms = pd.read_csv('./spam_clean.csv', encoding="latin-1")
sms

"""# Dataset Pre-processing and Analysis"""

results = Counter()
sms['message'].str.lower().str.split().apply(results.update)
print("Whole dataset has {} unique words".format(len(results)))
print(sum(results.values())/len(results), "average repetitions")
print(len([x for x in results if results[x] > 5]), "number of words that repeat more than 5 times")

"""## Tokenization"""

tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(sms["message"])

# tokenizer.fit_on_texts(x_train["message"])
word_index = tokenizer.word_index

# train_token = tokenizer.texts_to_sequences(x_train['message'])
# test_token = tokenizer.texts_to_sequences(x_test['message'])
# val_token = tokenizer.texts_to_sequences(x_val['message'])

sms_token = tokenizer.texts_to_sequences(sms['message'])

index_word = {v: k for k, v in tokenizer.word_index.items()} 
print(index_word)

# print(sms["message"].copy().reset_index(inplace=False)["message"][0])
# for w in sms_token[0]:
#     x = index_word[w]
#     print(x,end = ' ')

sms['token'] = sms['message'].apply(lambda x: tokenizer.texts_to_sequences([x])[0])
sms['token_len'] = sms['token'].apply(len)
sms

sms['token_len'].describe()

# Removing sentences with too little tokenized words

res = sms[sms.token_len > minlen]
res

max(sms.token_len)

# all_tokens = train_token+test_token
# avg = sum(map(len, all_tokens))/len(all_tokens)
# std = np.sqrt(sum( map(lambda x: (len(x) - avg)**2, all_tokens)) / len(all_tokens))
# longest = max(map(len, all_tokens))

# print(avg,std, longest)

# binary_embedding = np.random.binomial(1, 0.5, (vocab_size, 1000))

"""# Splitting the dataset"""

x_train, x_val, y_train, y_val = train_test_split(res, res['spam'], test_size=validation_split, random_state=111)
x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=train_split/(train_split+test_split), random_state=111)

print("Shape of training data {}".format(x_train.shape))
print("Shape of testing data {}".format(x_test.shape))
print("Shape of validation data {}".format(x_val.shape))

x_train.head(5)

x_test.head(5)

"""# Embedding Matrix

## GloVe 200
"""

# Downloading GloVe

!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip
!unzip glove*.zip

# Loading GloVe 200

embeddings_index = {} # We create a dictionary of word -> embedding
f = open('./glove.6B.200d.txt', encoding='utf-8')
# In file f, each line represents a new word embedding
# The line starts with the word and the embedding values follow
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Found %s word vectors.' % len(embeddings_index))

# Create a matrix of all embeddings
# np.stack() joins a sequence of arrays along a new axis, here into a matrix
# .values() returns a list of all the values in the dictionary, here coeffs
all_embs = np.stack(list(embeddings_index.values())) # Join a sequence of arrays along a new axis.
print(all_embs.max(), all_embs.min())
print(all_embs.shape)
emb_mean = all_embs.mean() # Calculate mean
emb_std = all_embs.std() # Calculate standard deviation
print(emb_mean,emb_std)

embedding_dim= all_embs.shape[1]

# Create a random matrix with the same mean and std as the embeddings
# It is used to randomly choose the embedding vectors 
#     for the words that are in the dataset but not in word-vector representation dataset
embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, embedding_dim))

# Loop over all words in the word index
for word, i in word_index.items():
    # If we are above the amount of words we want to use we do nothing
    if i >= vocab_size: 
        continue
    # Get the embedding vector for the word
    embedding_vector = embeddings_index.get(word)
    # If there is an embedding vector, put it in the embedding matrix
    if embedding_vector is not None: 
        embedding_matrix[i] = embedding_vector

"""## Random Embedding"""

emb_mean = -0.008671866
emb_std = 0.38186193
embedding_dim = 200
rdm_embedding = np.random.normal(emb_mean, emb_std, (vocab_size, embedding_dim))
print("Shape of random embedding", rdm_embedding.shape)
# print("First random embedding", rdm_embedding[0])
# print("Another random embedding", rdm_embedding[100])

"""# Padding"""

from tensorflow.keras.preprocessing.sequence import pad_sequences

train_token = x_train.reset_index(inplace=False)["token"]
test_token = x_test.reset_index(inplace=False)["token"]
val_token = x_val.reset_index(inplace=False)["token"]
longest = np.argmax(list(map(len, train_token)))
print("Longest tokenized sentence in training set:\n", x_train.reset_index(inplace=False)["message"][longest])
print("Tokenized:\n", train_token[longest])
print("Tokenized (in words):\n", " ".join([index_word[x] for x in train_token[longest]]))
shortest = np.argmin(list(map(len, train_token)))
print("\n")
print("Shortest tokenized sentence in training set:\n", x_train.reset_index(inplace=False)["message"][shortest])
print("Tokenized:\n", train_token[shortest])
print("Tokenized (in words):\n", " ".join([index_word[x] for x in train_token[shortest]]))

maxlen = max(map(len, train_token))
maxlen

pad_to = (maxlen + minlen) // 2

train_token_pad = pad_sequences(train_token, maxlen=pad_to, padding='pre')
test_token_pad = pad_sequences(test_token, maxlen=pad_to, padding='pre')
val_token_pad = pad_sequences(val_token, maxlen=pad_to, padding='pre')
print('Shape of train data:', train_token_pad.shape)
print('Shape of train labels:',y_train.shape)
print('Shape of test data:', test_token_pad.shape)
print('Shape of test labels:', y_test.shape)
print('Shape of test data:', val_token_pad.shape)
print('Shape of test labels:', y_val.shape)

print("Longest padded sentence:\n", train_token_pad[longest])
print("Shortest padded sentence:\n", train_token_pad[shortest])

"""# Preparing S (secret data), P (public data), Q (validation data), and F (random encoder)

## TensorFlow
"""

input_shape = train_token_pad.shape[1]
emb = Embedding(vocab_size, embedding_dim,
        weights=[rdm_embedding], input_length=input_shape, trainable=False, mask_zero=True)
ran_lstm = LSTM(100, return_sequences = False, kernel_initializer='random_normal', bias_initializer='zeros', trainable=False)

tf_train_x = emb(train_token_pad)
tf_randomized = ran_lstm(tf_train_x)
tf_test_x = emb(test_token_pad)
tf_randomized_test = ran_lstm(tf_test_x)
tf_val_x = emb(val_token_pad)
tf_randomized_val = ran_lstm(tf_val_x)
print("embedded S shape", tf_train_x.shape)
print("embedded P shape", tf_test_x.shape)
print("embedded Q shape", tf_val_x.shape)
print("F(S) shape",tf_randomized.shape)
print("F(P) shape", tf_randomized_test.shape)
print("F(Q) shape",tf_randomized_val.shape)

input_shape = train_token_pad.shape[1]
emb = Embedding(vocab_size, embedding_dim,
        weights=[embedding_matrix], input_length=input_shape, trainable=False, mask_zero=True)
ran_lstm = LSTM(100, return_sequences = False, kernel_initializer='random_normal', bias_initializer='zeros', trainable=False)

tfglv_train_x = emb(train_token_pad)
tfglv_randomized = ran_lstm(tfglv_train_x)
tfglv_test_x = emb(test_token_pad)
tfglv_randomized_test = ran_lstm(tfglv_test_x)
tfglv_val_x = emb(val_token_pad)
tfglv_randomized_val = ran_lstm(tfglv_val_x)
print("Glove")
print("embedded S shape", tfglv_train_x.shape)
print("embedded P shape", tfglv_test_x.shape)
print("embedded Q shape", tfglv_val_x.shape)
print("F(S) shape",tfglv_randomized.shape)
print("F(P) shape", tfglv_randomized_test.shape)
print("F(Q) shape",tfglv_randomized_val.shape)

"""## PyTorch"""

# TODO: pass in key as argument
class RLSTM(nn.Module):
    def __init__(self):
        super(RLSTM, self).__init__()
        self.LSTM = nn.LSTM(200,100, batch_first=True)
        # nn.init.normal_(self.LSTM.weight_ih_l0, 0, 0.05)
        # nn.init.constant_(self.LSTM.bias_ih_l0, 0)
    def forward(self, x):
        return self.LSTM(x.float())

class EmbRLSTM(nn.Module):
    def __init__(self):
        super(EmbRLSTM, self).__init__()
        self.emb_layer = nn.Embedding.from_pretrained(torch.tensor(rdm_embedding))
        self.LSTM = nn.LSTM(200,100, batch_first=True)
        nn.init.normal_(self.LSTM.weight_ih_l0, 0, 0.05)
        nn.init.constant_(self.LSTM.bias_ih_l0, 0)
    def forward(self, x):
        x = emb_layer(x)
        return self.LSTM(x)

# f = RLSTM()
# f(train_x[0].unsqueeze(0).float())[0]

# f(train_x[0].unsqueeze(0).float())[0]

# g = EmbRLSTM()
# g(train_token.to_numpy())

# embedding layer
emb_layer = nn.Embedding.from_pretrained(torch.tensor(rdm_embedding))

# embedded S, P, Q
train_x = emb_layer(torch.LongTensor(train_token_pad))
test_x = emb_layer(torch.LongTensor(test_token_pad))
val_x = emb_layer(torch.LongTensor(val_token_pad))

# random LSTM F
f = RLSTM()
# f = nn.LSTM(200, 100, batch_first=True)
# nn.init.normal_(f.weight_ih_l0, mean=0, std=0.05)
# nn.init.constant_(f.bias_ih_l0, 0)
print(f.LSTM.weight_ih_l0[0][1])

# F(S), F(Q)
randomized = f(train_x.float())
randomized_test = f(test_x.float())
randomized_val = f(val_x.float())
print(f.LSTM.weight_ih_l0[0][1])

# embedding layer
glv_emb_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix))

# embedded S, P, Q
glv_train_x = emb_layer(torch.LongTensor(train_token_pad))
glv_test_x = emb_layer(torch.LongTensor(test_token_pad))
glv_val_x = emb_layer(torch.LongTensor(val_token_pad))

# random LSTM F
f = RLSTM()
# f = nn.LSTM(200, 100, batch_first=True)
# nn.init.normal_(f.weight_ih_l0, mean=0, std=0.05)
# nn.init.constant_(f.bias_ih_l0, 0)
print(f.LSTM.weight_ih_l0[0][1])

# F(S), F(Q)
glv_randomized = f(glv_train_x.float())
glv_randomized_test = f(glv_test_x.float())
glv_randomized_val = f(glv_val_x.float())
print(f.LSTM.weight_ih_l0[0][1])

print("embedded S shape", train_x.shape)
print("embedded P shape", test_x.shape)
print("embedded Q shape", val_x.shape)
print("F(S) shape", randomized[1][0].shape)
print("F(Q) shape", randomized_val[1][0].shape)

print("section of emb(S)", train_x[0][-1], train_x[1][-1])
# print("section of emb(P)", test_x[0][0], test_x[1][0])
# print("section of emb(Q)", val_x[0][0][:5], val_x[1][0][:5])
print("first 5 F(S)", randomized[1][0][:5])
print("first 5 F(P)", randomized_test[1][0][:5])
print("first 5 F(Q)", randomized_val[1][0][:5])

o, (h, c) = randomized
print(o.shape)
print(h.shape)
print(c.shape)

print(o[:][:][-1])

print(c[:][:][-1])

np.mean(randomized[1][0].detach().numpy(), axis=0)

# print(train_token)
# print(np.array(train_token))
# _ = emb_layer(torch.from_numpy(np.array(train_token)))
# f(torch.LongTensor(_))

# print(type(train_token_pad))

"""# Reconstruction Attack (Estimating F)

## TensorFlow
"""

# def attack(input_shape):
#     sequence_input = Input(shape=input_shape[1:])
#     # Homa: Is this F' or F?
#     state = LSTM(100, return_sequences=False, recurrent_dropout=0.2)(sequence_input)
#     model = Model(sequence_input, state)
#     model.compile(loss=tf.keras.losses.KLDivergence(), optimizer='adam', metrics=['accuracy'])
#     print(model.summary())
#     return model

# recon = attack(train_x.shape) # Homa: 100?
# #idx = np.random.randint(3900, size=...)
# #np.array(randomized)[idx, :]
# recon.fit(test_x, randomized)

# y_pred = recon.predict(val_x)
# # val_recon = tf.keras.metrics.MeanSquaredError()
# # val_recon.update_state(y_pred, randomized_val)
# # print(val_recon.result().numpy())
# val_recon_mse = tf.keras.metrics.mean_squared_error(y_pred, randomized_val) # Homa: The other operand should be F(Q)
# print("Validation MSE of attack", val_recon_mse)

"""## PyTorch"""

class ALSTM(nn.Module):
    def __init__(self):
        super(ALSTM, self).__init__()
        self.LSTM = nn.LSTM(200,100, batch_first=True)
        # nn.init.normal_(self.LSTM.weight_ih_l0, 0, 0.05)
        # nn.init.constant_(self.LSTM.bias_ih_l0, 0)
    def forward(self, x):
        return self.LSTM(x.float())

"""# MMD Loss [PyTorch]"""

SIGMAS = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1,
        1, 5, 10, 15, 20, 25, 30, 35, 100,
        1e3, 1e4, 1e5, 1e6]
sigmas = torch.nn.Parameter(torch.FloatTensor(SIGMAS), requires_grad=False)

def compute_pairwise_distances(x, y):
    """ Computes the squared pairwise Euclidean distances between x and y.
    Args:
      x: a tensor of shape [num_x_samples, num_features]
      y: a tensor of shape [num_y_samples, num_features]
    Returns:
      a distance matrix of dimensions [num_x_samples, num_y_samples]
    Raise:
      ValueError: if the inputs do no matched the specified dimensions.
    """
    if not len(x.size()) == len(y.size()) == 2:
        raise ValueError('Both inputs should be matrices.')
    if x.size()[1] != y.size()[1]:
        raise ValueError('The number of features should be the same.')

    # By making the `inner' dimensions of the two matrices equal to 1 using
    # broadcasting then we are essentially substracting every pair of rows
    # of x and y.
    norm = lambda x: torch.sum(x * x, 1)
    return norm(x.unsqueeze(2) - y.t())

def gaussian_kernel(x, y, sigmas):
    """ Computes a Gaussian RBK between the samples of x and y.
    We create a sum of multiple gaussian kernels each having a width sigma_i.
    Args:
      x: a tensor of shape [num_samples, num_features]
      y: a tensor of shape [num_samples, num_features]
      sigmas: a tensor of floats which denote the widths of each of the
        gaussians in the kernel.
    Returns:
      A tensor of shape [num_samples{x}, num_samples{y}] with the RBF kernel
    """
    beta = 1. / (2. * (sigmas.unsqueeze(1)))

    dist = compute_pairwise_distances(x, y)

    s = torch.matmul(beta, dist.view(1, -1))
    return (torch.sum(torch.exp(-s), 0)).view_as(dist)
def mmd_loss(source, target):
    if eat_ram:
      source = source[:100, :]
      target = target[:100, :]
    cost = torch.mean(gaussian_kernel(source, source, sigmas))
    cost += torch.mean(gaussian_kernel(target, target, sigmas))
    cost -= 2 * torch.mean(gaussian_kernel(source, target, sigmas))

    cost = torch.clamp(cost, min=0)
    return cost

fprime = ALSTM()

criterion = mmd_loss
optimizer = optim.SGD(fprime.parameters(), lr=0.001, momentum=0.9)

for _ in tqdm.tqdm(range(epochs)):
  optimizer.zero_grad()
  outputs = fprime(glv_test_x) # F'(P)
  # print("F'(P) shape", outputs[1][0].squeeze(0).shape)
  # print("F(S) shape", randomized[1][0].squeeze(0).shape)
  loss = criterion(outputs[1][0].squeeze(0), glv_randomized[1][0].squeeze(0)) #F'(P), F(S)
  loss.backward(retain_graph=True)
  optimizer.step()
  print("MMD Loss in training F'", loss.item())

"""<h3> MSE Loss Validation </h3>"""

loss = nn.MSELoss()
outputs_val = fprime(glv_val_x)
print("F(Q) shape", glv_randomized_val[1][0].shape)
print("F'(Q) shape", outputs_val[1][0].shape)
print("Validation MSE of smart attack", loss(glv_randomized_val[1][0], outputs_val[1][0]))

while True:
  pass

randomized[1][0]

randomized_val[1][0]

outputs_val[1][0]

"""# Naive attack

## TensorFlow
"""

# idx = np.random.randint(2507, size=558)
# stupid_result = np.mean(randomized, axis=0)
# stupid_result = (np.ones((np.array(randomized_val).shape[0], 1))) * stupid_result 

# # val_stupid_mse = tf.keras.metrics.mean_squared_error(stupid_result, randomized_val)
# # print(val_stupid_mse)

# # val_stupid_recon = tf.keras.metrics.MeanSquaredError() # Homa: one not the same as above
# # val_stupid_recon.update_state(stupid_result , randomized_val)
# # print("Validation MSE of stupid attack", val_stupid_recon.result().numpy())

"""## PyTorch"""

rdm = glv_randomized[1][0].squeeze(0).detach().numpy()
stupid_result = np.mean(rdm, axis=0)
stupid_result = (np.ones((glv_randomized_val[1][0].shape[1], 1))) * stupid_result

print("F(Q) shape", glv_randomized_val[1][0].squeeze(0).shape)
print("stupid shape", torch.LongTensor(stupid_result).shape)
print("Validation MSE of stupid attack", loss(glv_randomized_val[1][0].squeeze(0), torch.LongTensor(stupid_result)))

print(stupid_result[0])

"""# Utility"""

def utility_dnn(input_shape):
  sequence_input = Input(shape=(input_shape)) 
  state = Dense(100, activation='relu')(sequence_input)
  l_dense1d = Dropout(0.1)(state)
  dense = Dense(1, activation='sigmoid')(l_dense1d)
  model = Model(sequence_input, dense)
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  model.summary()
  return model

train_x.detach().numpy().shape

u_dnn = utility_dnn(train_x.shape[1:])
u_dnn.fit(train_x.detach().numpy(), y_train)
y_pred = u_dnn.predict(test_x.detach().numpy())

y_pred.shape

u_dnn = utility_dnn(randomized[1][0].shape[-1])
u_dnn.fit(randomized[1][0].squeeze(0).detach().numpy(), y_train, epochs = 10)

y_pred = u_dnn.predict(randomized_test[1][0].squeeze(0).detach().numpy())

y_pred

accuracy_score(tf.round(y_pred), y_test)

n_dnn = utility_dnn(train_token_pad.shape[1:])
n_dnn.fit(train_token_pad, y_train, epochs=10)
y_pred_n = n_dnn.predict(test_token_pad)
accuracy_score(tf.round(y_pred_n), y_test)

u_dnn = utility_dnn(tfglv_randomized.shape[1:])
u_dnn.fit(tfglv_randomized, y_train, epochs = 10)
y_pred = u_dnn.predict(tf_randomized_test)
accuracy_score(tf.round(y_pred), y_test)

def emb_rlstm_dnn(input_shape, emb_matrix):
  sequence_input = Input(shape=(input_shape)) 
  emb = Embedding(vocab_size, embedding_dim, 
                  weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
  ran_lstm = LSTM(100, return_sequences = True, kernel_initializer=tf.keras.initializers.RandomNormal(mean=emb_mean, stddev=emb_std), bias_initializer='zeros', trainable=False)(emb)
  state = Dense(100, activation='relu')(ran_lstm)
  l_dense1d = Dropout(0.1)(state)
  dense = Dense(1, activation='sigmoid')(l_dense1d)
  model = Model(sequence_input, dense)
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  model.summary()
  return model

u_dnn = emb_rlstm_dnn(train_token_pad.shape[1], rdm_embedding)
u_dnn.fit(train_token_pad, y_train, epochs = 10)
y_pred = u_dnn.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)

u_dnn = emb_rlstm_dnn(train_token_pad.shape[1], embedding_matrix)
u_dnn.fit(train_token_pad, y_train, epochs = 10)
y_pred = u_dnn.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)

udnn = utility_dnn(tfglv_randomized.shape[1])
udnn.fit(tfglv_randomized, y_train, epochs = 10)
y_pred = udnn.predict(tfglv_randomized_test)
accuracy_score(tf.round(y_pred), y_test)

"""##PyTorch"""





"""#Uncleaned"""

def reverse_attack(input_shape):
    sequence_input = Input(shape=input_shape[1:])
    state = Dense(169*200)(sequence_input)
    res = Reshape((169, 200))(state)
    model = Model(sequence_input, res)
    model.compile(loss=tf.keras.losses.KLDivergence(), optimizer='adam', metrics=['accuracy'])
    print(model.summary())
    return model

rev = reverse_attack(randomized.shape)
rev.fit(randomized, test_x)
y_pred = rev.predict(randomized_test)
m = tf.keras.metrics.KLDivergence()
m.update_state(test_x, y_pred)
m.result().numpy()

n = tf.keras.metrics.KLDivergence()
n.update_state(y_pred, )

def recon_nodense(input_shape, emb_matrix):
    sequence_input = Input(shape=input_shape[1:])
#     bn = BatchNormalization()(sequence_input)
    state = LSTM(200, return_sequences=True, recurrent_dropout=0.2)(sequence_input)
#     onehot = tf.one_hot(state, 800)
    rev = tf.linalg.matmul(state,tf.linalg.pinv(emb.weights[0]))
#     rev = tf.keras.layers.Multiply()([state, tf.linalg.pinv(emb.weights[0])])
    softmax = tf.keras.layers.Softmax()(rev)
# model = Model(sequence_input, dense)
    
    model = Model(sequence_input, softmax)
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
#     model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])
    print(model.summary())
    return model

lstm_ = recon_nodense(randomized.shape, train_token_pad)
lstm_.summary()
lstm_.fit(randomized, onehot_train)

preds = lstm_.predict(randomized)
temp = np.argmax(preds, axis = -1)
temp

sum(temp)

train_token_pad

sum(train_token_pad)

def tf_count(t, val):
    elements_equal_to_value = tf.equal(t, val)
    as_ints = tf.cast(elements_equal_to_value, tf.int32)
    count = tf.reduce_sum(as_ints)
    return count

tf_count(train_token_pad, 0)

3900*169

def random_lstm_recon(input_shape, emb_matrix):
    sequence_input = Input(shape=input_shape[1:])
#     bn = BatchNormalization()(sequence_input)
    state = LSTM(100, return_sequences=True, recurrent_dropout=0.2)(sequence_input)
    dense = Dense(800, activation='softmax')(state)
# model = Model(sequence_input, dense)
    
    model = Model(sequence_input, dense)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
#     model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])
    print(model.summary())
    return model

lstm_ = random_lstm_recon(randomized.shape, rdm_embedding)
lstm_.summary()
lstm_.fit(randomized, onehot_train)
# y_pred = lstm_.predict(bn_random_test)
# ((y_pred - test_token_pad)**2).numpy().mean()

preds = lstm_.predict(randomized)
temp = np.argmax(preds, axis=-1)
temp

preds.shape

print("MSE", ((y_pred - test_x)**2).numpy().mean())
print("predicted max", y_pred.max())
print("actual max", tf.reduce_max(test_x).numpy())
print("predicted std", tf.math.reduce_std(y_pred).numpy())
print("actual std", tf.math.reduce_std(test_x).numpy())
print("predicted mean", tf.reduce_mean(y_pred).numpy())
print("actual mean", tf.reduce_mean(test_x).numpy())

print("MSE", ((y_pred - test_x)**2).numpy().mean())
print("predicted max", y_pred.max())
print("actual max", tf.reduce_max(test_x).numpy())
print("predicted std", tf.math.reduce_std(y_pred).numpy())
print("actual std", tf.math.reduce_std(test_x).numpy())
print("predicted mean", tf.reduce_mean(y_pred).numpy())
print("actual mean", tf.reduce_mean(test_x).numpy())

lstm_ = random_lstm_recon(bn_random.shape, rdm_embedding)
lstm_.summary()
lstm_.fit(bn_random, train_x, callbacks=[time_callback], epochs=5)
y_pred = lstm_.predict(bn_random_test)
((y_pred - test_x)**2).numpy().mean()

print("MSE", ((y_pred - test_x)**2).numpy().mean())
print("predicted max", y_pred.max())
print("actual max", tf.reduce_max(test_x).numpy())
print("predicted std", tf.math.reduce_std(y_pred).numpy())
print("actual std", tf.math.reduce_std(test_x).numpy())
print("predicted mean", tf.reduce_mean(y_pred).numpy())
print("actual mean", tf.reduce_mean(test_x).numpy())

bin_emb = Embedding(vocab_size, 1000,
        weights=[binary_embedding], input_length=input_shape, trainable=False, mask_zero=True)
import sk_dsp_comm.digitalcom as dc
import sk_dsp_comm.fec_conv as fec
bin_train_x = bin_emb(train_token_pad)
bin_train_x

class TimeHistory(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.times = []

    def on_epoch_begin(self, batch, logs={}):
        self.epoch_time_start = time.time()

    def on_epoch_end(self, batch, logs={}):
        self.times.append(time.time() - self.epoch_time_start)

time_callback = TimeHistory()

def create_model(input_shape):
  sequence_input = Input(shape=input_shape) 
  # l_flat = Flatten()(sequence_input)
  l_dense1 = Dense(100, activation='relu')(sequence_input)
  l_dense1d = Dropout(0.1)(l_dense1)
  l_dense2 = Dense(100, activation='relu')(l_dense1d)
  l_dense2d = Dropout(0.1)(l_dense2)
  preds = Dense(1, activation='sigmoid')(l_dense2d)

  model = Model(sequence_input, preds)
  model.summary()

  return model

def evaluate(xtrain, xtest, model):
    model.fit(xtrain, y_train)
    return accuracy_score(model.predict(xtest), y_test)

def whole_package(xtrain, xtest, scale=False):
    if scale:
        print("LR accuracy:", evaluate(xtrain, xtest, make_pipeline(StandardScaler(), LogisticRegression())))
        print("SVC accuracy:", evaluate(xtrain, xtest, make_pipeline(StandardScaler(), SVC())))
    else:
        print("LR accuracy:", evaluate(xtrain, xtest, LogisticRegression()))
        print("SVC accuracy:", evaluate(xtrain, xtest, SVC()))
    dnn = create_model(xtrain.shape[1])
    optimizer = Adam(lr=0.0001, decay=1e-6)
    dnn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    history = dnn.fit(xtrain, y_train, epochs=400, batch_size=256,callbacks=[time_callback])  
    score = dnn.evaluate(xtest, y_test)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])

evaluate(average_train, average_test, LogisticRegression(class_weight="balanced"))

evaluate(average_train, average_test, SVC(class_weight="balanced", verbose=True))

whole_package(average_train, average_test)

original_NN_val_times = np.cumsum(time_callback.times)
original_NN_val_times[-1]

def lstm(input_shape, emb_matrix):
    sequence_input = Input(shape=input_shape) 
    emb = Embedding(vocab_size, embedding_dim,
        weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
    state = LSTM(100, return_state=False, recurrent_dropout=0.2)(emb)
    dense = Dense(1, activation='sigmoid')(state)
#     l_dense1 = Dense(100, activation='relu')(state)
#     l_dense1d = Dropout(0.1)(l_dense1)
#     l_dense2 = Dense(100, activation='relu')(l_dense1d)
#     l_dense2d = Dropout(0.1)(l_dense2)
#     dense = Dense(1, activation='sigmoid')(l_dense2d)
    model = Model(sequence_input, dense)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
# acc = 0
# for i in range(5):
lstm_ = lstm(train_token_pad.shape[1], embedding_matrix)
lstm_.summary()
lstm_.fit(train_token_pad, y_train, callbacks=[time_callback])
y_pred = lstm_.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)
#     acc+=accuracy_score(tf.round(y_pred), y_test)
# print(acc/5)

train_token_pad.shape

# acc = 0
# for i in range(5):
# lstm_nostop = lstm(train_token_pad_nostop.shape[1], embedding_matrix_nostop)
# lstm_nostop.summary()
# lstm_nostop.fit(train_token_pad_nostop, y_train, callbacks=[time_callback])
# y_pred_nostop = lstm_nostop.predict(test_token_pad_nostop)
# accuracy_score(tf.round(y_pred_nostop), y_test)
#     acc += accuracy_score(tf.round(y_pred_nostop), y_test)
# print(acc/5)

def lstm_model(input_shape, emb_matrix):
    sequence_input = Input(shape=input_shape) 
    emb = Embedding(vocab_size, embedding_dim,
        weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
    states = LSTM(100, return_state=True, recurrent_dropout=0.2)(emb)
    state = states[2]
    dense = Dense(1, activation='sigmoid')(state)
    model = Model(sequence_input, dense)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# acc = 0
# for i in range(5):
lstm_ = lstm_model(train_token_pad.shape[1], embedding_matrix)
lstm_.summary()
lstm_.fit(train_token_pad, y_train, callbacks=[time_callback])
y_pred = lstm_.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)
#     acc += accuracy_score(tf.round(y_pred), y_test)
# print(acc/5)

acc = 0
# for i in range(5):
# lstm_nostop = lstm_model(train_token_pad_nostop.shape[1], embedding_matrix_nostop)
# lstm_nostop.summary()
# lstm_nostop.fit(train_token_pad_nostop, y_train, callbacks=[time_callback])
# y_pred_nostop = lstm_nostop.predict(test_token_pad_nostop)
# accuracy_score(tf.round(y_pred_nostop), y_test)
#     acc+=accuracy_score(tf.round(y_pred_nostop), y_test)
# print(acc/5)

def lstm_sequence_model(input_shape, emb_matrix):
    sequence_input = Input(shape=input_shape) 
    emb = Embedding(vocab_size, embedding_dim,
        weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
    states = LSTM(100, return_sequences=True, recurrent_dropout=0.2)(emb)
    print(states.shape)
    state = states[1]
    dense = Dense(1, activation='sigmoid')(state)
    model = Model(sequence_input, dense)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

from tensorflow.keras import initializers
def random_lstm(input_shape, emb_matrix):
    sequence_input = Input(shape=input_shape) 
    emb = Embedding(vocab_size, embedding_dim,
    weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
    ran_lstm = LSTM(100, return_sequences = True, kernel_initializer='random_normal', bias_initializer='zeros', trainable=False)(emb)
    state = LSTM(100, return_sequences=False, recurrent_dropout=0.2)(ran_lstm)
    dense = Dense(1, activation='sigmoid')(state)
    model = Model(sequence_input, dense)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model

acc = 0
# for i in range(5):
lstm_ = random_lstm(train_token_pad.shape[1], embedding_matrix)
lstm_.summary()
lstm_.fit(train_token_pad, y_train, callbacks=[time_callback])
y_pred = lstm_.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)
# acc+=accuracy_score(tf.round(y_pred), y_test)
# acc

cc1 = fec.FECConv(('11101','10011'),25)
# x = np.random.randint(0,2,10000)
y,state=cc1.conv_encoder(bin_train_x[0][0], '0000')

# y,state= cc1.conv_encoder(bin_train_x, '0000')

def random_random_lstm(input_shape, emb_matrix):
    sequence_input = Input(shape=input_shape) 
    emb = Embedding(vocab_size, embedding_dim,
        weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
    ran_lstm = LSTM(100, return_sequences = True, kernel_initializer='random_normal', bias_initializer='random_normal', trainable=False)(emb)
    state = LSTM(100, return_sequences=False, recurrent_dropout=0.2)(ran_lstm)
    dense = Dense(1, activation='sigmoid')(state)
    model = Model(sequence_input, dense)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model

lstm_ = random_random_lstm(train_token_pad.shape[1], embedding_matrix)
lstm_.fit(train_token_pad, y_train, callbacks=[time_callback])
y_pred = lstm_.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)

def random_lstm_dnn(input_shape, emb_matrix):
    sequence_input = Input(shape=input_shape) 
    emb = Embedding(vocab_size, embedding_dim,
        weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
    ran_lstm = LSTM(100, kernel_initializer='random_normal', bias_initializer='zeros', trainable=False)(emb)
    state = Dense(100, activation='relu')(ran_lstm)
    l_dense1d = Dropout(0.1)(state)
    dense = Dense(1, activation='sigmoid')(l_dense1d)
    model = Model(sequence_input, dense)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model

lstm_ = random_lstm_dnn(train_token_pad.shape[1], embedding_matrix)
lstm_.fit(train_token_pad, y_train, callbacks=[time_callback])
y_pred = lstm_.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)

def random_lstm_stddev(input_shape, emb_matrix):
    sequence_input = Input(shape=input_shape) 
    emb = Embedding(vocab_size, embedding_dim,
        weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
    ran_lstm = LSTM(100, return_sequences = True, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=0.5), bias_initializer='zeros', trainable=False)(emb)
    state = LSTM(100, return_sequences=False, recurrent_dropout=0.2)(ran_lstm)
    dense = Dense(1, activation='sigmoid')(state)
    model = Model(sequence_input, dense)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model
lstm_ = random_lstm_stddev(train_token_pad.shape[1], embedding_matrix)
lstm_.fit(train_token_pad, y_train, callbacks=[time_callback])
y_pred = lstm_.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)

def random_lstm_stddev(input_shape, emb_matrix):
    sequence_input = Input(shape=input_shape) 
    emb = Embedding(vocab_size, embedding_dim,
        weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
    ran_lstm = LSTM(100, return_sequences = True, kernel_initializer=tf.keras.initializers.RandomNormal(mean=emb_mean, stddev=emb_std), bias_initializer='zeros', trainable=False)(emb)
    state = LSTM(100, return_sequences=False, recurrent_dropout=0.2)(ran_lstm)
    dense = Dense(1, activation='sigmoid')(state)
    model = Model(sequence_input, dense)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model
lstm_ = random_lstm_stddev(train_token_pad.shape[1], embedding_matrix)
lstm_.fit(train_token_pad, y_train, callbacks=[time_callback])
y_pred = lstm_.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)

rglstm_ = lstm(train_token_pad.shape[1], rdm_embedding)
rglstm_.fit(train_token_pad, y_train, callbacks=[time_callback])
y_pred = rglstm_.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)

rglstm_ = random_lstm(train_token_pad.shape[1], rdm_embedding)
rglstm_.fit(train_token_pad, y_train, callbacks=[time_callback])
y_pred = rglstm_.predict(test_token_pad)
accuracy_score(tf.round(y_pred), y_test)

# def random_lstm(input_shape, emb_matrix):
#     sequence_input = Input(shape=input_shape) 
#     emb = Embedding(vocab_size, embedding_dim,
#         weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
#     ran_lstm = LSTM(100, return_sequences = True, kernel_initializer='random_normal', bias_initializer='zeros', trainable=False)(emb)
#     state = LSTM(100, return_sequences=False, recurrent_dropout=0.2)(ran_lstm)
#     dense = Dense(1, activation='sigmoid')(state)
#     model = Model(sequence_input, dense)
#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
#     model.summary()
#     return model

rns = [0.05, 0.5, 1, emb_std]
rus = [0.05, 0.5, 1, emb_std]
tries = [tf.keras.initializers.RandomNormal(mean=0, stddev=i) for i in rns] + [tf.keras.initializers.RandomUniform(minval=-i, maxval=i) for i in rus] 
labels = ["Normal std {}".format(i) for i in rns] + ["Uniform values in range [-{}, {}]".format(i, i) for i in rus]
def random_lstms(input_shape, emb_matrix):
    models = []
    for i in tries:
        sequence_input = Input(shape=input_shape) 
        emb = Embedding(vocab_size, embedding_dim,
            weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
        ran_lstm = LSTM(100, return_sequences = True, kernel_initializer=i, bias_initializer='zeros', trainable=False)(emb)
        state = LSTM(100, return_sequences=False, recurrent_dropout=0.2)(ran_lstm)
        dense = Dense(1, activation='sigmoid')(state)
        model = Model(sequence_input, dense)
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        model.summary()
        models.append(model)
    return models

accs = [0 for i in range(len(tries))]
for i in range(5):
    models = random_lstms(train_token_pad.shape[1], embedding_matrix)
    for idx, model in enumerate(models):
        model.fit(train_token_pad, y_train, callbacks=[time_callback])
        y_pred = model.predict(test_token_pad)
        accs[i] += accuracy_score(tf.round(y_pred), y_test)
        print(labels[idx], accuracy_score(tf.round(y_pred), y_test))

rns = [0.05, 0.5, 1, emb_std]
rus = [0.05, 0.5, 1, emb_std]
tries = [tf.keras.initializers.RandomNormal(mean=0, stddev=i) for i in rns] + [tf.keras.initializers.RandomUniform(minval=-i, maxval=i) for i in rus] 
labels = ["Normal std {}".format(i) for i in rns] + ["Uniform values in range [-{}, {}]".format(i, i) for i in rus]
def random_dnns(input_shape, emb_matrix):
    models = []
    for i in tries:
        sequence_input = Input(shape=input_shape) 
        emb = Embedding(vocab_size, embedding_dim,
            weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
        ran_lstm = LSTM(100, return_sequences = False, kernel_initializer=i, bias_initializer='zeros', trainable=False)(emb)
        l_dense1 = Dense(100, activation='relu')(ran_lstm)
        l_dense1d = Dropout(0.1)(l_dense1)
        l_dense2 = Dense(100, activation='relu')(l_dense1d)
        l_dense2d = Dropout(0.1)(l_dense2)
        preds = Dense(1, activation='sigmoid')(l_dense2d)
        model = Model(sequence_input, preds)
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        model.summary()
        models.append(model)
    return models
models = random_dnns(train_token_pad.shape[1], embedding_matrix)
for idx, model in enumerate(models):
    model.fit(train_token_pad, y_train, callbacks=[time_callback])
    y_pred = model.predict(test_token_pad)
    print(labels[idx], accuracy_score(tf.round(y_pred), y_test))

rns = [0.05, 0.5, 1]
rus = [0.05, 0.5, 1]
tries = [tf.keras.initializers.RandomNormal(mean=0, stddev=i) for i in rns] + [tf.keras.initializers.RandomUniform(minval=-i, maxval=i) for i in rus] 
labels = ["Normal std {}".format(i) for i in rns] + ["Uniform values in range [-{}, {}]".format(i, i) for i in rus]
def random_bn_lstms(input_shape, emb_matrix):
    models = []
    for i in tries:
        sequence_input = Input(shape=input_shape) 
        emb = Embedding(vocab_size, embedding_dim,
            weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
        ran_lstm = LSTM(100, return_sequences = True, kernel_initializer=i, bias_initializer='zeros', trainable=False)(emb)
        bn = BatchNormalization()(ran_lstm)
        state = LSTM(100, return_sequences=False, recurrent_dropout=0.2)(bn)
        dense = Dense(1, activation='sigmoid')(state)
        model = Model(sequence_input, dense)
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        model.summary()
        models.append(model)
    return models

accs = [0 for i in range(len(tries))]
for i in range(5):
    models = random_bn_lstms(train_token_pad.shape[1], embedding_matrix)
    for idx, model in enumerate(models):
        model.fit(train_token_pad, y_train, callbacks=[time_callback])
        y_pred = model.predict(test_token_pad)
        accs[idx] += accuracy_score(tf.round(y_pred), y_test)
for idx in range(len(tries)):
    print(labels[idx], accs[idx]/5)

rns = [0.05, 0.5, 1]
rus = [0.05, 0.5, 1]
tries = [tf.keras.initializers.RandomNormal(mean=0, stddev=i) for i in rns] + [tf.keras.initializers.RandomUniform(minval=-i, maxval=i) for i in rus] 
labels = ["Normal std {}".format(i) for i in rns] + ["Uniform values in range [-{}, {}]".format(i, i) for i in rus]
def random_bn_dnns(input_shape, emb_matrix):
    models = []
    for i in tries:
        sequence_input = Input(shape=input_shape) 
        emb = Embedding(vocab_size, embedding_dim,
            weights=[emb_matrix], input_length=input_shape, trainable=False, mask_zero=True)(sequence_input)
        ran_lstm = LSTM(100, return_sequences = False, kernel_initializer=i, bias_initializer='zeros', trainable=False)(emb)
        bn = BatchNormalization()(ran_lstm)
        l_dense1 = Dense(100, activation='relu')(bn)
        l_dense1d = Dropout(0.1)(l_dense1)
        l_dense2 = Dense(100, activation='relu')(l_dense1d)
        l_dense2d = Dropout(0.1)(l_dense2)
        preds = Dense(1, activation='sigmoid')(l_dense2d)
        model = Model(sequence_input, preds)
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        model.summary()
        models.append(model)
    return models

accs = [0 for i in range(len(tries))]
for i in range(5):
    models = random_bn_dnns(train_token_pad.shape[1], embedding_matrix)
    for idx, model in enumerate(models):
        model.fit(train_token_pad, y_train, callbacks=[time_callback])
        y_pred = model.predict(test_token_pad)
        accs[idx] += accuracy_score(tf.round(y_pred), y_test)
for idx in range(len(tries)):
    print(labels[idx], accs[idx]/5)

